[
    {
        "email": "storchaka@gmail.com",
        "date": 1559314933000.0,
        "content": "Subject: [Python-Dev] Expected stability of PyCode_New() and\n types.CodeType() signatures\n\n\n31.05.19 11:46, Petr Viktorin ????:\n> PEP 570 (Positional-Only Parameters) changed the signatures of \n> PyCode_New() and types.CodeType(), adding a new argument for \"posargcount\".\n> Our policy for such changes seems to be fragmented tribal knowledge. I'm \n> writing to check if my understanding is reasonable, so I can apply it \n> and document it explicitly.\n> \n> There is a surprisingly large ecosystem of tools that create code objects.\n> The expectation seems to be that these tools will need to be adapted for \n> each minor version of Python.\n\nI have a related proposition. Yesterday I have reported two bugs (and \nPablo quickly fixed them) related to handling positional-only arguments. \nThese bugs were occurred due to subtle changing the meaning of \nco_argcount. When we make some existing parameters positional-only, we \ndo not add new arguments, but mark existing parameters. But co_argcount \nnow means the only number of positional-or-keyword parameters. Most code \nwhich used co_argcount needs now to be changed to use \nco_posonlyargcount+co_argcount.\n\nI propose to make co_argcount meaning the number of positional \nparameters (i.e. positional-only + positional-or-keyword). This would \nremove the need of changing the code that uses co_argcount.\n\nAs for the code object constructor, I propose to make posonlyargcount an \noptional parameter (default 0) added after existing parameters. \nPyCode_New() can be kept unchanged, but we can add new PyCode_New2() or \nPyCode_NewEx() with different signature.\n\n"
    },
    {
        "email": "solipsis@pitrou.net",
        "date": 1559323832000.0,
        "content": "Subject: [Python-Dev] PEP 595: Improving bugs.python.org\n\n\nOn Fri, 31 May 2019 11:58:22 -0700\nNathaniel Smith <njs at pobox.com> wrote:\n> On Fri, May 31, 2019 at 11:39 AM Barry Warsaw <barry at python.org> wrote:\n> >\n> > On May 31, 2019, at 01:22, Antoine Pitrou <solipsis at pitrou.net> wrote:\n> >  \n> > > I second this.\n> > >\n> > > There are currently ~7000 bugs open on bugs.python.org.  The Web UI\n> > > makes a good job of actually being able to navigate through these bugs,\n> > > search through them, etc.\n> > >\n> > > Did the Steering Council conduct a usability study of Github Issues\n> > > with those ~7000 bugs open?  If not, then I think the acceptance of\n> > > migrating to Github is a rushed job.  Please reconsider.  \n> >\n> > Thanks for your feedback Antoine.\n> >\n> > This is a tricky issue, with many factors and tradeoffs to consider.  I really appreciate Ezio and Berker working on PEP 595, so we can put all these issues on the table.\n> >\n> > I think one of the most important tradeoffs is balancing the needs of existing developers (those who actively triage bugs today), and future contributors.  But this and other UX issues are difficult to compare on our actual data right now.  I fully expect that just as with the switch to git, we?ll do lots of sample imports and prototyping to ensure that GitHub issues will actually work for us (given our unique requirements), and to help achieve the proper balance.  It does us no good to switch if we just anger all the existing devs.\n> >\n> > IMHO, if the switch to GH doesn?t improve our workflow, then it definitely warrants a reevaluation.  I think things will be better, but let?s prove it.  \n> \n> Perhaps we should put an explicit step on the transition plan, after\n> the prototyping, that's \"gather feedback from prototypes, re-evaluate,\n> make final go/no-go decision\"? I assume we'll want to do that anyway,\n> and having it formally written down might reassure people. It might\n> also encourage more people to actually try out the prototypes if we\n> make it very clear that they're going to be asked for feedback.\n\nIndeed, regardless of the exact implementation details, I think \"try\nfirst, decide after\" is the right procedure here.\n\nRegards\n\nAntoine.\n"
    },
    {
        "email": "pablogsal@gmail.com",
        "date": 1559336974000.0,
        "content": "Subject: [Python-Dev] Expected stability of PyCode_New() and\n types.CodeType() signatures\nMessage-ID: <CAFjbc8GsB20m0K-ZbBoiMdiG0WjbS81s=XbTnHFNGTDjPdvA1g@mail.gmail.com>\n\n>\n> I propose to make co_argcount meaning the number of positional\n> parameters (i.e. positional-only + positional-or-keyword). This would\n> remove the need of changing the code that uses co_argcount.\n>\n\nI like the proposal, it will certainly make handling normal cases\ndownstream much easier because\nif you do not care about positional-only arguments you can keep\ninspecting co_argcount\nand that\nwill give you what you expect. Note that if we choose to do this, it has to\nbe done now-ish IMHO to\navoid making the change painful because it will change the semantics of\nco_argcount.\n\n\n> As for the code object constructor, I propose to make posonlyargcount an\n> optional parameter (default 0) added after existing parameters.\n> PyCode_New() can be kept unchanged, but we can add new PyCode_New2() or\n> PyCode_NewEx() with different signature.\n\n\nI am not convinced about having a default argument in the code constructor.\nThe code constructor\nis kept with all arguments positional for efficiency and adding defaults\nwill make it slower or having\na more confusing an asymmetrical interface. Also, this will be misaligned\non how keyword-only\nparameters are provided. This is by far not the first time this constructor\nhas changed.\n\nOn the Python side, the new code.replace should cover most of the\nPython-side use cases regarding\ncreating code objects from the Python side.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://mail.python.org/pipermail/python-dev/attachments/20190601/8b1827fb/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: license.dash-license\nType: application/octet-stream\nSize: 693 bytes\nDesc: not available\nURL: <http://mail.python.org/pipermail/python-dev/attachments/20190601/8b1827fb/attachment.obj>\n"
    },
    {
        "email": "stefan_ml@behnel.de",
        "date": 1559338089000.0,
        "content": "Subject: [Python-Dev] Expected stability of PyCode_New() and\n types.CodeType() signatures\n\n\nSerhiy Storchaka schrieb am 01.06.19 um 09:02:\n> I have a related proposition. Yesterday I have reported two bugs (and Pablo\n> quickly fixed them) related to handling positional-only arguments. These\n> bugs were occurred due to subtle changing the meaning of co_argcount. When\n> we make some existing parameters positional-only, we do not add new\n> arguments, but mark existing parameters. But co_argcount now means the only\n> number of positional-or-keyword parameters. Most code which used\n> co_argcount needs now to be changed to use co_posonlyargcount+co_argcount.\n> \n> I propose to make co_argcount meaning the number of positional parameters\n> (i.e. positional-only + positional-or-keyword). This would remove the need\n> of changing the code that uses co_argcount.\n\nSounds reasonable to me. The main distinction points are positional\narguments vs. keyword arguments vs. local variables. Whether the positional\nones are positional or positional-only is irrelevant in many cases.\n\n\n> PyCode_New() can be kept unchanged, but we can add new PyCode_New2() or\n> PyCode_NewEx() with different signature.\n\nIt's not a commonly used function, and it's easy for C code to adapt. I\ndon't think it's worth adding a new function to the C-API here, compared to\njust changing the signature. Very few users would benefit, at the cost of\nadded complexity.\n\nStefan\n\n"
    },
    {
        "email": "pablogsal@gmail.com",
        "date": 1559346932000.0,
        "content": "Subject: [Python-Dev] Expected stability of PyCode_New() and\n types.CodeType() signatures\nMessage-ID: <CAFjbc8E+zdp096fVFBaNnD7UFT6feiTr0sPQkdytD7KtYy3Cnw@mail.gmail.com>\n\nOpened https://bugs.python.org/issue37122 to track this in the bug tracker.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://mail.python.org/pipermail/python-dev/attachments/20190601/633b348e/attachment.html>\n"
    },
    {
        "email": "tim.peters@gmail.com",
        "date": 1559397412000.0,
        "content": "Subject: [Python-Dev] obmalloc (was Have a big machine and spare time?\n Here's a possible Python bug.)\n\n\n[Antoine Pitrou, replying to Thomas Wouters]\n> Interesting that a 20-year simple allocator (obmalloc) is able to do\n> better than the sophisticated TCMalloc.\n\nIt's very hard to beat obmalloc (O) at what it does.  TCMalloc (T) is\nactually very similar where they overlap, but has to be more complex\nbecause it's trying to do more than O.\n\nIn either case, for small objects \"the fast path\" consists merely of\ntaking the first block of memory off a singly-linked size-segregated\nfree list.  For freeing, the fast path is just linking the block back\nto the front of the appropriate free list.  What _could_ be faster?  A\n\"bump allocator\" allocates faster (just increment a highwater mark),\nbut creates worlds of problems when freeing.\n\nBut because O is only trying to deal with small (<= 512 bytes)\nrequests, it can use a very fast method based on trivial address\narithmetic to find the size of an allocated block by just reading it\nup from the start of the (4K) \"pool\" the address belongs to.  T can't\ndo that - it appears to need to look up the address in a more\nelaborate radix tree, to find info recording the size of the block\n(which may be just about anything - no upper limit).\n\n> (well, of course, obmalloc doesn't have to worry about concurrent\n> scenarios, which explains some of the simplicity)\n\nRight, T has a different collection of free lists for each thread. so\non each entry has to figure out which collection to use (and so\ndoesn't need to lock).  That's not free.  O only has one collection,\nand relies on the GIL.\n\nAgainst that, O burns cycles worrying about something else:  because\nit was controversial when it was new, O thought it was necessary to\nhandle free/realloc calls even when passed addresses that had actually\nbeen obtained from the system malloc/realloc.  The T docs I saw said\n\"don't do that - things will blow up in mysterious ways\".\n\nThat's where O's excruciating \"address_in_range()\" logic comes from.\nWhile that's zippy and scales extremely well (it doesn't depend on how\nmany objects/arenas/pools exist), it's not free, and is a significant\npart of the \"fast path\" expense for both allocation and deallocation.\n\nIt also limits us to a maximum pool size of 4K (to avoid possible\nsegfaults when reading up memory that was actually obtained from the\nsystem malloc/realloc), and that's become increasingly painful:  on\n64-bit boxes the bytes lost to pool headers increased, and O changed\nto handle requests up to 512 bytes instead of its original limit of\n256.  O was intended to supply \"a bunch\" of  usable blocks per pool,\nnot just a handful.  We \"should\" really at least double the pool and\narena sizes now.\n\nI don't think we need to cater anymore to careless code that mixes\nsystem memory calls with O calls (e.g., if an extension gets memory\nvia `malloc()`, it's its responsibility to call `free()`), and if not\nthen `address_in_range()` isn't really necessary anymore either, and\nthen we could increase the pool size.  O would, however, need a new\nway to recognize when its version of malloc punted to the system\nmalloc.\n\nBTW, one more:  last I saw T never returns memory to \"the system\", but\nO does - indeed, the parent thread here was all about _enormous_ time\nwaste due to that in O ;-)  That's not free either, but doesn't affect\nO's fast paths.\n"
    },
    {
        "email": "ezio.melotti@gmail.com",
        "date": 1559406282000.0,
        "content": "Subject: [Python-Dev] PEP 595: Improving bugs.python.org\n\n\nOn Sat, Jun 1, 2019 at 11:50 AM Antoine Pitrou <solipsis at pitrou.net> wrote:\n>\n> On Fri, 31 May 2019 11:58:22 -0700\n> Nathaniel Smith <njs at pobox.com> wrote:\n> > On Fri, May 31, 2019 at 11:39 AM Barry Warsaw <barry at python.org> wrote:\n> > >\n> > > On May 31, 2019, at 01:22, Antoine Pitrou <solipsis at pitrou.net> wrote:\n> > >\n> > > > I second this.\n> > > >\n> > > > There are currently ~7000 bugs open on bugs.python.org.  The Web UI\n> > > > makes a good job of actually being able to navigate through these bugs,\n> > > > search through them, etc.\n> > > >\n> > > > Did the Steering Council conduct a usability study of Github Issues\n> > > > with those ~7000 bugs open?  If not, then I think the acceptance of\n> > > > migrating to Github is a rushed job.  Please reconsider.\n> > >\n> > > Thanks for your feedback Antoine.\n> > >\n> > > This is a tricky issue, with many factors and tradeoffs to consider.  I really appreciate Ezio and Berker working on PEP 595, so we can put all these issues on the table.\n> > >\n> > > I think one of the most important tradeoffs is balancing the needs of existing developers (those who actively triage bugs today), and future contributors.\n\nThese can be further divided in several groups: from core devs and\nrelease managers, to triagers, to regular and occasional contributors,\nto people that just want to report an issue and be done with it, to\npeople that think the error they just got is a Python bug, each of\nthem with different goals and needs.\n\nI think that rather than discussing whether GitHub Issues is better or\nworse than Roundup, we should first try to understand who is facing\nwhat issues now, and who will face what issues after the switch.  This\ncan be done both by gathering feedback from different types of people\nand by testing and comparing the solutions (see below).\n\nOnce we know what the issues are, we should evaluate if and how we can\naddress them, and also -- if we can't make everyone happy -- what\ngroups of people we want to prioritize (e.g. do we want core devs to\nbe more effective at dealing with the thousands of already existing\nissues, or we want to make it easier for users to report new bugs?).\n\n> > >  But this and other UX issues are difficult to compare on our actual data right now.  I fully expect that just as with the switch to git, we?ll do lots of sample imports and prototyping to ensure that GitHub issues will actually work for us (given our unique requirements), and to help achieve the proper balance.  It does us no good to switch if we just anger all the existing devs.\n> > >\n> > > IMHO, if the switch to GH doesn?t improve our workflow, then it definitely warrants a reevaluation.  I think things will be better, but let?s prove it.\n> >\n> > Perhaps we should put an explicit step on the transition plan, after\n> > the prototyping, that's \"gather feedback from prototypes, re-evaluate,\n> > make final go/no-go decision\"? I assume we'll want to do that anyway,\n> > and having it formally written down might reassure people. It might\n> > also encourage more people to actually try out the prototypes if we\n> > make it very clear that they're going to be asked for feedback.\n>\n> Indeed, regardless of the exact implementation details, I think \"try\n> first, decide after\" is the right procedure here.\n>\n\nTesting a change of this magnitude is not trivial.  I can see several\npossible options:\n* using the on-demand approach proposed by PEP 588, a full migration,\nor some other solution (e.g. parallel, synced trackers);\n* doing a throwaway test migration (import zero/some/all existing\nissues, then discard any new message/issue at the end of the test) or\nusing real issues directly (import zero/some/all issues and keep\nadding real messages/issues);\n  * if we do a test migration and it works, we might need to do a\nsecond, real migration, possibly involving the GH staff twice; if it\ndoesn't work, we discard everything and that's it;\n  * if we use real issues, we might need to migrate things back to\nRoundup if GH doesn't fit our needs and it might be confusing for\nusers;\n* starting from scratch on GH with new issues (at least initially, for\ntesting purposes) or porting some/all issues from bpo;\n  * if we start from scratch we don't need to write the tools to\nmigrate, but we won't have feedback about searching/navigating through\nlot of issues;\n  * if we port some/all the issues, we need to write the tools to do\nit, even if it's just for testing purposes and we end going back to\nRoundup;\n* limiting the test to triagers/core-devs, or involve regular users;\n  * if we involve regular users we might get better feedback, but\nthere's risk of confusion (afaik the only way to inform users on\nGitHub Issues is writing another bot that adds messages) and backlash;\n* doing separate specific tests (e.g. having a read-only repo with all\nthe issues to test search/navigation, and a separate read-write repo\nto test issue creation) or a \"real-world\" test;\n  * some specific tests might be easier to setup (e.g. issue creation\ntest using templates), but for others we still need to import some/all\nthe issues;\n\nIf we agree on testing, I think we need to discuss the options, define\nand document a list of steps, and start working on it.\n\nBest Regards,\nEzio Melotti\n\n> Regards\n>\n> Antoine.\n"
    },
    {
        "email": "solipsis@pitrou.net",
        "date": 1559410626000.0,
        "content": "Subject: [Python-Dev] obmalloc (was Have a big machine and spare time?\n Here's a possible Python bug.)\nReferences: <CAExdVNmP=LHAXbXmhsecq=S9dAEVb6UYykDQ=xUcO1zJVh1Pfw@mail.gmail.com>\n <CAEfz+TxM-_viFaaQgqFbzoON0VnN74dpyJsP4Db=Ui-wj9=J-w@mail.gmail.com>\n <CA+LW3i1w8AcjCjRjyGkJyLwdmiQZxTLv_AMxUtUJrPjQk6MYQA@mail.gmail.com>\n <CAEfz+TwcyHnOisiPc6qmNFfPSS63+Ts_6H08jKGCnFuTTk9frQ@mail.gmail.com>\n <64d3f69a-b900-d17d-679e-aa748d0a23ab@python.org>\n <CAPdQG2rr978QuKb08YBeiHCxGPbnmm9A3vTU3-_nvOngM976nw@mail.gmail.com>\n <20190526112415.33e4a02d@fsol>\n <CAExdVN=7VmELOvv-BfW-JL8yFEMVTLt=yyBsFNTWB7pugE9HmQ@mail.gmail.com>\nMessage-ID: <20190602113706.01c14820@fsol>\n\nOn Sun, 2 Jun 2019 00:56:52 -0500\nTim Peters <tim.peters at gmail.com> wrote:\n> \n> But because O is only trying to deal with small (<= 512 bytes)\n> requests, it can use a very fast method based on trivial address\n> arithmetic to find the size of an allocated block by just reading it\n> up from the start of the (4K) \"pool\" the address belongs to.  T can't\n> do that - it appears to need to look up the address in a more\n> elaborate radix tree, to find info recording the size of the block\n> (which may be just about anything - no upper limit).\n\nThe interesting thing here is that in many situations, the size is\nknown up front when deallocating - it is simply not communicated to the\ndeallocator because the traditional free() API takes a sole pointer,\nnot a size.  But CPython could communicate that size easily if we\nwould like to change the deallocation API.  Then there's no bother\nlooking up the allocated size in sophisticated lookup structures.\n\nI'll note that jemalloc provides such APIs:\nhttp://jemalloc.net/jemalloc.3.html\n\n\"\"\"The dallocx() function causes the memory referenced by ptr to be\nmade available for future allocations.\n\nThe sdallocx() function is an extension of dallocx() with a size\nparameter to allow the caller to pass in the allocation size as an\noptimization.\"\"\"\n\nRegards\n\nAntoine.\n\n\n> \n> > (well, of course, obmalloc doesn't have to worry about concurrent\n> > scenarios, which explains some of the simplicity)  \n> \n> Right, T has a different collection of free lists for each thread. so\n> on each entry has to figure out which collection to use (and so\n> doesn't need to lock).  That's not free.  O only has one collection,\n> and relies on the GIL.\n> \n> Against that, O burns cycles worrying about something else:  because\n> it was controversial when it was new, O thought it was necessary to\n> handle free/realloc calls even when passed addresses that had actually\n> been obtained from the system malloc/realloc.  The T docs I saw said\n> \"don't do that - things will blow up in mysterious ways\".\n> \n> That's where O's excruciating \"address_in_range()\" logic comes from.\n> While that's zippy and scales extremely well (it doesn't depend on how\n> many objects/arenas/pools exist), it's not free, and is a significant\n> part of the \"fast path\" expense for both allocation and deallocation.\n> \n> It also limits us to a maximum pool size of 4K (to avoid possible\n> segfaults when reading up memory that was actually obtained from the\n> system malloc/realloc), and that's become increasingly painful:  on\n> 64-bit boxes the bytes lost to pool headers increased, and O changed\n> to handle requests up to 512 bytes instead of its original limit of\n> 256.  O was intended to supply \"a bunch\" of  usable blocks per pool,\n> not just a handful.  We \"should\" really at least double the pool and\n> arena sizes now.\n> \n> I don't think we need to cater anymore to careless code that mixes\n> system memory calls with O calls (e.g., if an extension gets memory\n> via `malloc()`, it's its responsibility to call `free()`), and if not\n> then `address_in_range()` isn't really necessary anymore either, and\n> then we could increase the pool size.  O would, however, need a new\n> way to recognize when its version of malloc punted to the system\n> malloc.\n> \n> BTW, one more:  last I saw T never returns memory to \"the system\", but\n> O does - indeed, the parent thread here was all about _enormous_ time\n> waste due to that in O ;-)  That's not free either, but doesn't affect\n> O's fast paths.\n\n\n\n"
    },
    {
        "email": "armin.rigo@gmail.com",
        "date": 1559415812000.0,
        "content": "Subject: [Python-Dev] [PEP 558] thinking through locals() semantics\n\n\nHi,\n\nOn Wed, 29 May 2019 at 08:07, Greg Ewing <greg.ewing at canterbury.ac.nz> wrote:\n> Nick Coghlan wrote:\n> > Having a single locals() call de-optimize an entire function would be\n> > far from ideal.\n>\n> I don't see what would be so bad about that. The vast majority\n> of functions have no need for locals().\n\nYou have the occasional big function that benefits a lot from being\nJIT-compiled but which contains ``.format(**locals())``.  That occurs\nin practice, and that's why PyPy is happy that there is a difference\nbetween ``locals()`` and ``sys._getframe().f_locals``.  PyPy could be\nmade to support the full mutable view, but that's extra work that\nisn't done so far and is a bit unlikely to occur at this point.  It\nalso raises the significantly the efforts for other JIT\nimplementations of Python if they have to support a full-featured\n``locals()``; supporting ``_getframe().f_locals`` is to some extent\noptional, but supporting ``locals()`` is not.\n\n\nA bient?t,\n\nArmin.\n"
    },
    {
        "email": "greg.ewing@canterbury.ac.nz",
        "date": 1559418722000.0,
        "content": "Subject: [Python-Dev] [PEP 558] thinking through locals() semantics\n\n\nArmin Rigo wrote:\n> You have the occasional big function that benefits a lot from being\n> JIT-compiled but which contains ``.format(**locals())``.\n\nThere should be a lot less need for that now that we have f-strings.\n\n-- \nGreg\n"
    },
    {
        "email": "steve@pearwood.info",
        "date": 1559422277000.0,
        "content": "Subject: [Python-Dev] [PEP 558] thinking through locals() semantics\n\n\nOn Sun, Jun 02, 2019 at 11:52:02PM +1200, Greg Ewing wrote:\n> Armin Rigo wrote:\n> >You have the occasional big function that benefits a lot from being\n> >JIT-compiled but which contains ``.format(**locals())``.\n> \n> There should be a lot less need for that now that we have f-strings.\n\nI think you're forgetting that a lot of code (especially libraries) \neither have to support older versions of Python, and so cannot use \nf-strings at all, or was written using **locals before f-strings came \nalong, and hasn't been touched since.\n\nAnother case where f-strings don't help is when the template is \ndynamically generated.\n\nIt may be that there will be less new code written using **locals() but \nI don't think that the **locals() trick will disappear any time before \nPython 5000.\n\n\n-- \nSteven\n"
    },
    {
        "email": "python@mrabarnett.plus.com",
        "date": 1559430813000.0,
        "content": "Subject: [Python-Dev] [PEP 558] thinking through locals() semantics\n\n\nOn 2019-06-02 13:51, Steven D'Aprano wrote:\n> On Sun, Jun 02, 2019 at 11:52:02PM +1200, Greg Ewing wrote:\n>> Armin Rigo wrote:\n>> >You have the occasional big function that benefits a lot from being\n>> >JIT-compiled but which contains ``.format(**locals())``.\n>> \n>> There should be a lot less need for that now that we have f-strings.\n> \n> I think you're forgetting that a lot of code (especially libraries)\n> either have to support older versions of Python, and so cannot use\n> f-strings at all, or was written using **locals before f-strings came\n> along, and hasn't been touched since.\n> \n> Another case where f-strings don't help is when the template is\n> dynamically generated.\n> \n> It may be that there will be less new code written using **locals() but\n> I don't think that the **locals() trick will disappear any time before\n> Python 5000.\n> \nWe've had .format_map since Python 3.2, so why use \n``.format(**locals())`` instead of ``.format_map(locals())``?\n"
    },
    {
        "email": "random832@fastmail.com",
        "date": 1559439190000.0,
        "content": "Subject: [Python-Dev] [PEP 558] thinking through locals() semantics\n\n\nOn Wed, May 29, 2019, at 01:25, Nick Coghlan wrote:\n> Having a single locals() call de-optimize an entire function would be \n> far from ideal.\n\nWhat if there were a way to explicitly de-optimize a function, rather than guessing the user's intent based on looking for locals and exec calls (both of which are builtins which could be shadowed or assigned to other variables)?\n\nAlso, regardless of anything else, maybe in an optimized function locals should return a read-only mapping?\n"
    },
    {
        "email": "vstinner@redhat.com",
        "date": 1559440379000.0,
        "content": "Subject: [Python-Dev] Expected stability of PyCode_New() and\n types.CodeType() signatures\n\n\nLe vendredi 31 mai 2019, Simon Cross <hodgestar+pythondev at gmail.com> a\n?crit :\n> As the maintainer of Genshi, ...\n> The new CodeType.replace will remove some potential sources of breakages\nin the future, so thank you very much for adding that.\n\nHi Simon,\n\nYou're welcome :-) Genshi was one of my motivation to add\nCodeType.replace() ;-)\n\nVictor\n\n-- \nNight gathers, and now my watch begins. It shall not end until my death.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://mail.python.org/pipermail/python-dev/attachments/20190602/3e1921d5/attachment.html>\n"
    },
    {
        "email": "pablogsal@gmail.com",
        "date": 1559500738000.0,
        "content": "Subject: [Python-Dev] Recent buildbot reports and asyncio test failures\nMessage-ID: <CAFjbc8HwNvxJyC-AFEbXG5ktg5Oe9jB8F31RXfn7O5fuVkSSOA@mail.gmail.com>\n\nHi everyone,\n\nJust a heads-up regarding some messages you will see in your pull requests.\nThere is an intermittent failure on some buildbots\nregarding asyncio:\n\nhttps://buildbot.python.org/all/#/builders/21\n\nAs the builds do not fail all the time, the systems understand that if your\n(merged) commits fail to build, they may be the cause\nof the failure and then it does a report into the pull request.\n\nI am working on investigating a way to improve the report mechanism to make\nit less noisy in this case, but bear in mind that\nthe correct way to solve this is fixing the asyncio bug in the test suite\nand this won't likely go away completely until is solved.\n\nWe are doing all that we can to solve all the recent leaks and failures on\nthe test suite, but there is a noticeable increase in the\nnumber of merged pull requests because of the imminent feature freeze and\nbecause this happens across several timezones\nis very difficult to get them all.\n\nThanks to everyone that is helping solving these bugs :)\n\nRegards from sunny London,\nPablo Galindo Salgado\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://mail.python.org/pipermail/python-dev/attachments/20190603/bcd8def7/attachment.html>\n"
    },
    {
        "email": "vstinner@redhat.com",
        "date": 1559516719000.0,
        "content": "Subject: [Python-Dev] Recent buildbot reports and asyncio test failures\n\n\nMany tests are failing randomly on buildbots.\n\n*MANY* failures: \"test_asyncio: test_cancel_gather_2() dangling thread\"\nhttps://bugs.python.org/issue37137\n\nSome failures on specific buildbots. multiprocessing does crash\nrandomly on Windows:\n   https://bugs.python.org/issue37143\nand on FreeBSD:\n   https://bugs.python.org/issue37135\nI bet on a regression caused by:\n   https://bugs.python.org/issue33608#msg344240\n\nOne failure. \"test_asyncio timed out on AMD64 FreeBSD CURRENT Shared 3.x\"\nhttps://bugs.python.org/issue37142\n\nI failed to reproduce any of these bugs.\n\nVictor\n\n\nLe lun. 3 juin 2019 ? 12:42, Pablo Galindo Salgado\n<pablogsal at gmail.com> a ?crit :\n>\n> Hi everyone,\n>\n> Just a heads-up regarding some messages you will see in your pull requests. There is an intermittent failure on some buildbots\n> regarding asyncio:\n>\n> https://buildbot.python.org/all/#/builders/21\n>\n> As the builds do not fail all the time, the systems understand that if your (merged) commits fail to build, they may be the cause\n> of the failure and then it does a report into the pull request.\n>\n> I am working on investigating a way to improve the report mechanism to make it less noisy in this case, but bear in mind that\n> the correct way to solve this is fixing the asyncio bug in the test suite and this won't likely go away completely until is solved.\n>\n> We are doing all that we can to solve all the recent leaks and failures on the test suite, but there is a noticeable increase in the\n> number of merged pull requests because of the imminent feature freeze and because this happens across several timezones\n> is very difficult to get them all.\n>\n> Thanks to everyone that is helping solving these bugs :)\n>\n> Regards from sunny London,\n> Pablo Galindo Salgado\n> _______________________________________________\n> Python-Dev mailing list\n> Python-Dev at python.org\n> https://mail.python.org/mailman/listinfo/python-dev\n> Unsubscribe: https://mail.python.org/mailman/options/python-dev/vstinner%40redhat.com\n\n\n\n--\nNight gathers, and now my watch begins. It shall not end until my death.\n"
    },
    {
        "email": "vstinner@redhat.com",
        "date": 1559554510000.0,
        "content": "Subject: [Python-Dev] Recent buildbot reports and asyncio test failures\n\n\nHi,\n\nUpdate. I'm collaborating with Pablo to fix these recent regressions.\nWe had to revert 2 changes to fix asyncio and multiprocessing test\nfailures and crashes.\n\nNew issue:\n\n\"opcode cache for LOAD_GLOBAL emits false alarm in memory leak hunting\"\nhttps://bugs.python.org/issue37146\n\nINADA-san worked around this issue by disabling the new opcode cache\n(LOAD_GLOBAL optimization) when Python is compiled in debug mode.\n\nWe should try to find a more long term solution after beta1 release.\n\n\nLe lun. 3 juin 2019 ? 17:05, Victor Stinner <vstinner at redhat.com> a ?crit :\n> *MANY* failures: \"test_asyncio: test_cancel_gather_2() dangling thread\"\n> https://bugs.python.org/issue37137\n\nAndrew Svetlov identified the root issue, it's a side effect of a new feature:\nhttps://bugs.python.org/issue37137#msg344488\n\nHe reverted his change to fix the bug. We may reconsider the feature\nafter beta1 (that's up to our Release Manager, I guess).\n\nNote: I pushed 2 changes to attempt to fix the issue, but they didn't\nfix the issue. We can consider to revert them after beta1.\n\n\n> Some failures on specific buildbots. multiprocessing does crash\n> randomly on Windows:\n>    https://bugs.python.org/issue37143\n> and on FreeBSD:\n>    https://bugs.python.org/issue37135\n> I bet on a regression caused by:\n>    https://bugs.python.org/issue33608#msg344240\n\nPablo and me identified the root issue:\nhttps://bugs.python.org/issue37135#msg344509\n\nI reverted a change to fix the regression.\n\nSadly, previously I also reverted a different change which wasn't the\nroot cause. We can consider to maybe reapply it after beta1.\n\n(!) \"Night gathers, and now my watch begins. It shall not end until my\ndeath.\" (!)\n\nVictor\n"
    },
    {
        "email": "lukasz@langa.pl",
        "date": 1559630644000.0,
        "content": "Subject: [Python-Dev] [RELEASE] Python 3.8.0b1 is now available for testing\nMessage-ID: <2FEBF8A0-7D2D-4BB8-BE37-6B0E6D9F75B8@langa.pl>\n\nThe time has come for Python 3.8.0b1:\nhttps://www.python.org/downloads/release/python-380b1/ <https://www.python.org/downloads/release/python-380b1/>\nThis release is the first of four planned beta release previews. Beta release previews are intended to give the wider community the opportunity to test new features and bug fixes and to prepare their projects to support the new feature release. The next pre-release of Python 3.8 will be 3.8.0b2, currently scheduled for 2019-07-01.\n\n\nCall to action\n\nWe strongly encourage maintainers of third-party Python projects to test with 3.8 during the beta phase and report issues found to the Python bug tracker <https://bugs.python.org/> as soon as possible. While the release is planned to be feature complete entering the beta phase, it is possible that features may be modified or, in rare cases, deleted up until the start of the release candidate phase (2019-09-30). Our goal is have no ABI changes after beta 3 and no code changes after 3.8.0rc1, the release candidate. To achieve that, it will be extremely important to get as much exposure for 3.8 as possible during the beta phase.\n\nPlease keep in mind that this is a preview release and its use is not recommended for production environments.\n\n\nA new challenger has appeared!\n\nWith the release of Python 3.8.0b1, development started on Python 3.9. The ?master? branch in the cpython repository now tracks development of 3.9 while Python 3.8 received its own branch, called simply ?3.8?.\n\n\nAcknowledgments\n\nAs you might expect, creating new branches triggers a lot of changes in configuration for all sorts of tooling that we?re using. Additionally, the inevitable deadline for new features caused a flurry of activity that tested the buildbots to the max. The revert hammer got  used more than once.\n\nI would not be able to make this release available alone. Many thanks to the fearless duo of Pablo Galindo Salgado and Victor Stinner for spending tens of hours during the past week working on getting the buildbots green for release. Seriously, that took a lot of effort. We are all so lucky to have you both.\n\nThanks to Andrew Svetlov for his swift fixes to asyncio and to Yury Selivanov for code reviews, even when jetlagged. Thanks to Julien Palard for untangling the documentation configs. Thank you to Zachary Ware for help with buildbot and CI configuration. Thanks to Mariatta for helping with the bots. Thank you to Steve Dower for delivering the Windows installers.\n\nMost importantly though, huge thanks to Ned Deily who not only helped me understand the scope of this special release but also did some of the grunt work involved.\n\nLast but not least, thanks to you for making this release more meaty than I expected. There?s plenty of super exciting changes in there. Just take a look at ?What?s New <https://docs.python.org/3.8/whatsnew/3.8.html>?!\n\n\nOne more thing\n\nHey, fellow Core Developer, Beta 2 is in four weeks. If your important new feature got reverted last minute, or you decided not to merge due to inadequate time, I have a one time offer for you (restrictions apply). If you:\n\nfind a second core developer champion for your change; and\nin tandem you finish your change complete with tests and documentation before Beta 2\nthen I will let it in. I?m asking for a champion because it?s too late now for changes with hasty design or code review. And as I said, restrictions apply. For instance, at this point changes to existing APIs are unlikely to be accepted. Don?t start new work with 3.8 in mind. 3.9 is going to come sooner than you think!\n\n\n\n- ?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://mail.python.org/pipermail/python-dev/attachments/20190605/547d26e8/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://mail.python.org/pipermail/python-dev/attachments/20190605/547d26e8/attachment.sig>\n"
    },
    {
        "email": "vstinner@redhat.com",
        "date": 1559636489000.0,
        "content": "Subject: [Python-Dev] PEP 594: update 1\n\n\nSo what is happening for this PEP since Python 3.8 beta1 has been\nreleased? Is it too late for Python 3.8 or not?\n\nIt seems like most people are confused by the intent of the PEP. IMHO\nit would be better to rewrite \"Remove packages from the stdlib\" as\n\"Move some stdlib modules to PyPI\". But that would require to rewrite\nsome parts of the PEP to explain how modules are moved, who become the\nnew maintainers, how to support modules both in stdlib (old Python\nversions) and in PyPI (new Python), etc.\n\nVictor\n"
    },
    {
        "email": "tjreedy@udel.edu",
        "date": 1559642837000.0,
        "content": "Subject: [Python-Dev] PEP 594: update 1\n\n\nOn 6/4/2019 8:21 PM, Victor Stinner wrote:\n> So what is happening for this PEP since Python 3.8 beta1 has been\n> released? Is it too late for Python 3.8 or not?\n\nThe only action proposed for 3.8 was soft deprecation in the docs, which \nI presume can be done later in the beta process.\n\n> It seems like most people are confused by the intent of the PEP. IMHO\n> it would be better to rewrite \"Remove packages from the stdlib\" as\n> \"Move some stdlib modules to PyPI\". But that would require to rewrite\n> some parts of the PEP to explain how modules are moved, who become the\n> new maintainers, how to support modules both in stdlib (old Python\n> versions) and in PyPI (new Python), etc.\n> \n> Victor\n> \n\n\n-- \nTerry Jan Reedy\n\n\n\n"
    }
]